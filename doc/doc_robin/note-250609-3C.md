./core/src/authority_service.rs
---
`AuthorityService` 充当了验证者节点的 **网络服务端**。这意味着它负责**接收和处理**来自其他对等验证者节点的各种网络请求，例如发送区块、订阅区块流、获取缺失区块或提交等。

### 1. `AuthorityService` 结构体

* **作用**: `AuthorityService` 实现了 `NetworkService` trait，是共识模块中用于处理传入网络请求的具体逻辑。它与底层网络管理器（如 `AnemoManager` 或 `TonicManager`）集成，由后者负责将网络原始请求路由到 `AuthorityService` 对应的 `handle_` 方法。
* **类型参数 `C: CoreThreadDispatcher`**: 表明它需要一个实现了 `CoreThreadDispatcher` trait 的类型，用于与共识核心线程进行通信。
* **关键字段**:
    * `context: Arc<Context>`: 节点的全局上下文，提供委员会、参数、协议配置、度量和时钟等共享信息。
    * `block_verifier: Arc<dyn BlockVerifier>`: 区块验证器，用于验证传入区块的签名和内容。
    * `commit_vote_monitor: Arc<CommitVoteMonitor>`: 监控共识提交投票的组件，用于跟踪提交进展。
    * `synchronizer: Arc<SynchronizerHandle>`: 同步器句柄，用于在需要时触发同步流程（例如，当发现缺失祖先区块时）。
    * `core_dispatcher: Arc<C>`: 与共识核心线程通信的调度器，用于将处理过的区块添加到核心 DAG 状态。
    * `rx_block_broadcast: broadcast::Receiver<ExtendedBlock>`: 一个广播接收器，用于接收本节点（作为领导者）提议并广播的区块。这在 `handle_subscribe_blocks` 中用于创建流。
    * `subscription_counter: Arc<SubscriptionCounter>`: 计数器，用于跟踪有多少活跃的订阅流。
    * `transaction_certifier: TransactionCertifier`: 事务认证器，可能在区块被接受后处理事务。
    * `dag_state: Arc<RwLock<DagState>>`: DAG 状态的读写锁包装，维护内存中的 DAG 结构。
    * `round_tracker: Arc<RwLock<PeerRoundTracker>>`: 跟踪其他对等节点轮次状态的组件。


### 2. `impl NetworkService for AuthorityService<C>` 方法 (网络服务端接口实现)

#### 2.1 handle_send_block

```
async fn handle_send_block(
    &self, peer: AuthorityIndex, serialized_block: ExtendedSerializedBlock
) -> ConsensusResult<()>
```

* **作用**: 处理来自对等节点发送的单个区块请求。
* **逻辑**:
    * **故障点 (`fail_point_async!`)**: 在 RPC 响应前插入一个故障点，可能用于模糊测试或注入错误。
    * **反序列化**: 将接收到的 `ExtendedSerializedBlock` 反序列化为 `SignedBlock`。
    * **权限校验**: 检查区块的 `author` 是否与发送方 `peer` 匹配，不匹配则拒绝。
    * **区块验证**: 调用 `self.block_verifier.verify_and_vote()` 验证区块的签名和内容。如果验证失败，记录指标并返回错误。
    * **时间戳校验和等待**:
        * 如果协议配置**不**使用中位数时间戳，则检查区块时间戳是否太远未来 (`max_forward_time_drift`)。如果太远，拒绝区块。
        * 如果区块时间戳在未来（但未超过阈值），则会暂停当前任务 (`sleep(forward_time_drift)`) 直到区块时间戳变为当前时间，这是一种流量控制和防止时间跳跃攻击的机制。
        * 如果协议配置**使用**中位数时间戳，则只记录时间漂移指标，不进行等待或拒绝。
    * **提交投票监控**: 调用 `self.commit_vote_monitor.observe_block()` 观察区块中的提交投票，这有助于跟踪网络中已提交的区块。
    * **滞后提交检查**: 检查本节点的 `last_commit_index` 是否比 `quorum_commit_index` 滞后过多（超过 `commit_sync_batch_size * COMMIT_LAG_MULTIPLIER`）。如果是，则拒绝区块，以避免过多内存开销。
    * **指标记录**: 记录验证成功的区块数量 (`verified_blocks`) 和被拒绝的未来区块或滞后区块数量。
    * **Fastpath (快速路径)**: 如果协议配置启用了 `mysticeti_fastpath`，会将验证后的区块传递给 `transaction_certifier`。
    * **添加到 DAG**: 调用 `self.core_dispatcher.add_blocks()` 将已验证的区块添加到共识核心的 DAG 中。如果发现缺少祖先区块，则返回缺失的 `BlockRef` 集合。
    * **获取缺失祖先**: 如果存在缺失祖先，调用 `self.synchronizer.fetch_blocks()` 在后台异步获取这些缺失的区块。
    * **处理 `excluded_ancestors`**:
        * 从 `ExtendedSerializedBlock` 中获取 `excluded_ancestors`，并进行反序列化。
        * 对 `excluded_ancestors` 的数量进行限制。
        * 更新 `round_tracker`。
        * 记录 `network_received_excluded_ancestors_from_authority` 等指标。
        * 检查这些排除的祖先区块是否缺失，如果缺失，再次通过 `synchronizer` 异步获取。

#### 2.2 handle_subscribe_blocks
```handle_subscribe_blocks
async fn handle_subscribe_blocks(
    &self, peer: AuthorityIndex, last_received: Round
) -> ConsensusResult<BlockStream>
```
* **作用**: 处理来自对等节点的区块订阅请求。
* **逻辑**:
    * **故障点**: 插入一个故障点。
    * **获取缺失区块**: 从 `dag_state` 的缓存中获取 `last_received` 轮次之后本节点自己提议的区块（`missed_blocks`）。这确保订阅者能补上它可能错过的历史区块。
    * **创建广播流**: `BroadcastedBlockStream::new()`: 创建一个包装了 `rx_block_broadcast.resubscribe()` 的流。这个流会持续接收本节点未来广播的区块。
    * **合并流**: 使用 `missed_blocks.chain(broadcasted_blocks.map(ExtendedSerializedBlock::from))` 将历史缺失区块流和实时广播区块流合并为一个统一的 `BlockStream`。
    * 返回这个合并后的 `BlockStream`，供订阅的对等节点消费。

#### 2.3 handle_fetch_blocks
```
async fn handle_fetch_blocks(
    &self, peer: AuthorityIndex,
    block_refs: Vec<BlockRef>,
    highest_accepted_rounds: Vec<Round>,
    breadth_first: bool
) -> ConsensusResult<Vec<Bytes>>
```
* **作用**: 处理按引用获取区块的请求。
* **逻辑**:
    * **条件分支**: 根据 `self.context.protocol_config.consensus_batched_block_sync()` 是否启用，决定是调用旧版 `handle_fetch_blocks_old` 还是执行新的批量同步逻辑。
    * **新版逻辑**:
        * 对请求的 `block_refs` 数量进行限制。
        * 验证请求的 `block_refs` 的有效性。
        * 从 `dag_state` 中获取请求的区块。
        * **根据 `breadth_first` 参数决定祖先获取策略**:
            * 如果 `breadth_first` 为 `true` (广度优先)：从请求的区块中获取唯一的缺失祖先区块（轮次高于对等节点已知最高接受轮次的），并随机选择一部分添加到返回结果中，避免一次返回过多。
            * 如果 `breadth_first` 为 `false` (深度优先)：计算每个请求验证者的最低缺失轮次，然后从 `dag_state` 缓存中获取该验证者在 `highest_accepted_round + 1` 到 `lowest_missing_round` 范围内的区块。
        * 将所有获取到的区块序列化为 `Bytes` 并返回。

#### 2.4 handle_fetch_commits
```
async fn handle_fetch_commits(
    &self, 
    _peer: AuthorityIndex,
    commit_range: CommitRange
) -> ConsensusResult<(Vec<TrustedCommit>, Vec<VerifiedBlock>)>
```
* **作用**: 处理按索引范围获取共识提交的请求。
* **逻辑**:
    * **故障点**: 插入故障点。
    * **范围限制**: 计算一个包含最大提交同步批次大小的提交范围。
    * **扫描提交**: 从 `self.store` 中扫描该范围内的所有 `TrustedCommit`。
    * **验证提交投票**: 循环检查返回的提交，验证其投票是否达到法定数量（`QuorumThreshold`）。如果某个提交的投票未达法定数量，则从列表中移除该提交。
    * **获取认证区块**: 找到最后一个达到法定投票的提交，并从 `self.store` 中读取其所有的认证区块引用（`read_commit_votes`），然后获取这些区块。
    * 返回筛选后的提交列表和这些认证区块。

#### 2.5 handle_fetch_latest_blocks
```
async fn handle_fetch_latest_blocks(
    &self,
    peer: AuthorityIndex, 
    authorities: Vec<AuthorityIndex>
) -> ConsensusResult<Vec<Bytes>>
```
* **作用**: 处理获取指定验证者最新区块的请求。
* **逻辑**:
    * **故障点**: 插入故障点。
    * **校验参数**: 验证请求的 `authorities` 列表的数量和有效性。
    * **获取最新区块**: 遍历 `authorities` 列表，从 `dag_state` 中获取每个验证者的最新区块（`get_last_block_for_authority`）。
    * **排除创世区块**: 不返回创世区块，因为其被视为没有实际提交内容。
    * 将所有获取到的区块序列化为 `Bytes` 并返回。


#### 2.6 handle_get_latest_rounds
```
async fn handle_get_latest_rounds(
    &self, 
    _peer: AuthorityIndex
) -> ConsensusResult<(Vec<Round>, Vec<Round>)
```
* **作用**: 获取本节点当前已知的所有验证者的最高已接收轮次和最高已接受轮次。
* **逻辑**:
    * **故障点**: 插入故障点。
    * **最高接收轮次**: 调用 `self.core_dispatcher.highest_received_rounds()` 获取来自 Core 模块的最高接收轮次。
    * **最高接受轮次**: 从 `dag_state` 中获取每个验证者的最后缓存区块，并提取其轮次作为最高接受轮次。
    * **自身轮次同步**: 将本节点自身的最高接受轮次同步到 `highest_received_rounds` 中，因为自己的区块不经过 dispatcher。
    * 返回这两个 `Vec<Round>`。

### 3. `SubscriptionCounter` 结构体

* **作用**: 用于原子地统计区块广播流的活跃订阅数量，并根据数量变化通知共识核心线程。
* **字段**:
    * `context`: 节点上下文。
    * `counter`: `parking_lot::Mutex<Counter>`，包含 `count` (总订阅数) 和 `subscriptions_by_authority` (每个验证者的订阅数)。
    * `dispatcher`: `Arc<dyn CoreThreadDispatcher>`，用于通知共识核心。
* **`increment(peer: AuthorityIndex) -> Result<(), ConsensusError>`**: 增加指定对等节点的订阅计数和总订阅计数。如果总订阅数从 0 变为 1，则通知 `Core` 模块 `set_subscriber_exists(true)`。
* **`decrement(peer: AuthorityIndex) -> Result<(), ConsensusError>`**: 减少指定对等节点的订阅计数和总订阅计数。如果某个对等节点的订阅数变为 0，则更新其指标。如果总订阅数变为 0，则通知 `Core` 模块 `set_subscriber_exists(false)`。

### 4. `BroadcastStream` 结构体

* **作用**: 封装了 `tokio::sync::broadcast::Receiver`，将其转换为一个 `Stream`，并对订阅的生命周期进行管理。它容忍消息滞后（lags），只会记录警告而不是报错。
* **字段**: `peer` (订阅者 ID)、`inner` (包装了 `broadcast::Receiver` 的可重用 Future)、`subscription_counter`。
* **`impl Stream for BroadcastStream<T>`**:
    * `poll_next()`: 核心流方法。它会循环调用 `inner.poll(cx)` 来从底层 `broadcast::Receiver` 获取下一个消息。
        * 如果收到 `Ok(item)`，返回 `Some(item)`。
        * 如果 `RecvError::Closed`，返回 `None` (流关闭)。
        * 如果 `RecvError::Lagged(n)` (消息滞后)，则记录警告并继续尝试接收下一个消息，**容忍滞后而不是立即报错**。
* **`impl Drop for BroadcastStream<T>`**: 在 `BroadcastStream` 被销毁时，会自动调用 `subscription_counter.decrement()`，通知 `SubscriptionCounter` 减少订阅计数。
* **`async fn make_recv_future(...)`**: 辅助函数，创建一个 Future 来等待 `broadcast::Receiver` 的下一个消息。

### 总结：

`AuthorityService` 是 Sui 共识验证者节点网络交互的核心枢纽。它提供了一套全面的 RPC 服务来处理来自其他节点的请求，实现了：

* **区块发送/订阅/获取**: 支持多种方式的区块传输。
* **提交同步**: 负责获取提交历史和投票信息。
* **状态同步**: 提供获取最新轮次和区块的能力。
* **严格的校验**: 对传入数据进行多层校验，包括权限、区块有效性、时间戳和提交滞后等。
* **资源管理**: 通过 `SubscriptionCounter` 监控订阅数量，并通过 `synchronizer` 异步处理缺失数据。
* **测试友好性**: 内置 `fail_point_async` 宏，并容忍 `BroadcastStream` 的滞后，便于模糊测试和模拟。

---
consensus/core/src/broadcaster.rs
---
### `Broadcaster` 模块的职责：

`Broadcaster` 的核心职责是**持续地将新的提议区块发送（广播）给网络中的所有其他对等验证者节点**。它专门处理网络通信中的可靠性、重试和流量控制，以确保即使在网络不稳定或对等节点暂时离线的情况下，重要的区块也能最终被接收到。

### 1. `Broadcaster` 结构体

* **作用**: 封装了广播器内部的状态和管理逻辑。
* **字段**:
    * `senders: JoinSet<()>`: 这是 `tokio` 提供的一个任务集合。`Broadcaster` 会为委员会中的**每个对等验证者**启动一个独立的异步任务（Future），这些任务会负责将区块推送到对应的对等节点。`JoinSet` 用于管理这些并发的发送任务。

### 2. `Broadcaster` 实现 (`impl Broadcaster`)

#### 2.1 构造函数
* **作用**: `Broadcaster` 的构造函数，用于创建和初始化广播器。
* **参数**:
    * `context: Arc<Context>`: 节点的全局上下文，包含委员会、自身索引等信息。
    * `network_client: Arc<C>`: 网络的客户端接口，用于实际发送区块（`NetworkClient` 实现了 `send_block` 方法）。
    * `signals_receiver: &CoreSignalsReceivers`: 共识核心的信号接收器，用于获取新的提议区块。
* **逻辑**:
    * 创建一个空的 `JoinSet` (`senders`)。
    * 遍历委员会中的所有验证者。
    * 对于除了**本节点**之外的每一个对等节点（`if index == context.own_index { continue; }`），都会 `spawn` （启动）一个独立的异步任务：`Self::push_blocks(...)`。这些任务就是负责向特定对等节点发送区块的。
    * 返回 `Broadcaster` 实例。

#### 2.2 stop
```pub(crate) fn stop(&mut self)```
* **作用**: 停止所有正在运行的区块发送任务，实现广播器的优雅关闭。
* **逻辑**: 调用 `self.senders.abort_all()`。这会向 `JoinSet` 中所有正在运行的任务发送中止信号，但不会等待它们实际退出，以加快关闭速度。

#### 2.3 push_blocks
```async fn push_blocks<C: NetworkClient>(...)```
* **作用**: 这是 `Broadcaster` 的核心异步任务，`new` 函数会为每个对等节点 `spawn` 一个这样的任务。它在一个无限循环中运行，负责将区块推送到**特定的目标对等节点**。
* **参数**: `context`, `network_client`, `rx_block_broadcast` (区块广播接收器), `peer` (目标对等节点的索引)。
* **逻辑**:
    * `peer_hostname`: 获取目标对等节点的主机名，用于日志和指标。
    * `last_block`: `Option<VerifiedBlock>`，用于存储最近一次发送的区块，以备重试。
    * `retry_timer`: `tokio::time::interval`，一个周期性计时器，用于定期重试发送 `last_block`。
    * `requests: FuturesUnordered::new()`: 用于管理并发发送区块的请求。`BROADCAST_CONCURRENCY` 限制了同时发送给一个对等节点的区块数量。
    * **主循环 (`loop`)**: 使用 `tokio::select!` 同时等待三个事件：
        1.  **`rx_block_broadcast.recv()` (接收新区块)**:
            * 从共识核心的广播通道接收新的 `ExtendedBlock`。
            * 如果通道关闭 (`RecvError::Closed`)，表示验证器正在关闭，任务退出。
            * 如果发生 `RecvError::Lagged`（接收器滞后），记录警告并继续（**容忍滞后**，不抛出错误，这是为了在负载高时保持活跃）。
            * 将接收到的区块添加到 `requests` 集合中，通过 `send_block` 协程发送。
            * 更新 `last_block` 为最新接收到的区块。
        2.  **`requests.next()` (发送请求完成)**:
            * 当发送一个区块的异步请求 (`send_block` 协程) 完成时，处理其结果。
            * **成功 (`Ok(Ok(_))`)**:
                * 计算 RTT (`now - start`)，并使用**指数衰减平均法**更新 `rtt_estimate`。这是一种简单的自适应超时机制。
                * 重置 `retry_timer`，避免立即重试刚刚成功发送的区块。
            * **超时 (`Err(Elapsed { .. })`)**:
                * 增加 `rtt_estimate` (乘以 `TIMEOUT_RTT_INCREMENT_FACTOR`)，以便下次使用更长的超时时间。
                * 将相同的 `block` 重新添加到 `requests` 集合中进行重试。
            * **网络错误 (`Ok(Err(_))`)**:
                * 将相同的 `block` 重新添加到 `requests` 集合中进行重试。
        3.  **`retry_timer.tick()` (重试计时器触发)**:
            * 当重试计时器触发时，如果当前没有正在发送的请求 (`requests.is_empty()`)，并且有 `last_block` 缓存，则将 `last_block` 重新添加到 `requests` 集合中进行发送。这确保即使长时间没有新区块产生，最后一个区块也会定期被重试发送，以保持与对等节点的状态同步。
    * **RTT 限制**: 在每次循环结束时，将 `rtt_estimate` 限制在 5ms 到 5s 之间，防止过小或过大。
    * **指标记录**: 更新 `broadcaster_rtt_estimate_ms` 指标。


#### 2.4 send_block
```async fn send_block<C: NetworkClient>(...)` (辅助函数)**:
    * **作用**: 封装了实际发送区块的网络请求。
    * **参数**: `network_client`, `peer`, `rtt_estimate`, `block`。
    * **逻辑**:
        * 记录发送开始时间 `start`。
        * 计算请求超时 `req_timeout` (基于 `rtt_estimate` 和 `TIMEOUT_THRESHOLD_MULTIPLIER`)。
        * 确保 `network_timeout` 不小于 `MIN_SEND_BLOCK_NETWORK_TIMEOUT`。
        * 调用 `network_client.send_block()` 发送区块，并使用 `tokio::time::timeout` 限制请求时间。
        * 如果发送失败（无论是超时还是网络错误），在返回结果前会短暂休眠 (`sleep_until(start + req_timeout)`)，作为一种退避策略，避免立即重试。

### 总结：

`Broadcaster` 的设计亮点包括：

* **并发发送**: 为每个对等节点创建独立任务，并使用 `FuturesUnordered` 管理并发的发送请求。
* **自适应重试**: 采用指数衰减 RTT 估算来动态调整网络请求的超时时间。
* **流量控制和健壮性**: 通过 `BROADCAST_CONCURRENCY` 限制并发请求数，通过 `last_block` 重试和 `RecvError::Lagged` 容忍机制来处理网络滞后和临时断开，确保消息的最终到达。
* **资源清理**: `stop()` 方法用于优雅地终止所有发送任务。


---
./core/src/synchronizer.rs
---
### `Synchronizer` 模块的职责：

`Synchronizer` 的主要职责是**确保节点拥有最新的区块数据**，特别是那些缺失的区块，以保持其 DAG 状态与网络同步。它通过两种主要机制来实现这一点：

1.  **显式请求（Live Synchronization）**：主动向特定对等节点请求缺失的区块，尤其是在处理新接收区块时发现祖先缺失的情况。
2.  **周期性请求（Periodic Synchronization）**：通过一个调度器定期请求那些未能通过显式请求获取的缺失区块。

### 核心常量和辅助结构：

* **`FETCH_BLOCKS_CONCURRENCY: usize = 5`**: 单个 Authority 可以同时进行的“获取区块”请求的最大数量。
* **`FETCH_REQUEST_TIMEOUT: Duration = Duration::from_millis(2_000)`**: 单个“获取区块”请求的网络超时时间。
* **`FETCH_FROM_PEERS_TIMEOUT: Duration = Duration::from_millis(4_000)`**: 从多个对等节点获取区块的总超时时间。
* **`MAX_AUTHORITIES_TO_FETCH_PER_BLOCK: usize = 2`**: 针对同一个缺失区块，最多允许多少个 Authority 同时去获取它。
* **`MAX_PERIODIC_SYNC_PEERS: usize = 3`**: 周期性同步时，最多同时向多少个对等节点请求缺失区块。
* **`BlocksGuard` 结构体**:
    * **作用**: 一个 RAII（Resource Acquisition Is Initialization）风格的守卫，用于管理对 `InflightBlocksMap` 中区块锁的生命周期。
    * **`Drop` 实现**: 当 `BlocksGuard` 实例离开作用域时，会自动调用 `map.unlock_blocks()` 方法，释放其持有的区块锁。这确保了即使请求失败或提前退出，区块锁也能被正确释放。
* **`InflightBlocksMap` 结构体**:
    * **作用**: 维护一个映射，记录当前有哪些区块正在被哪些 Authority 获取（即处于“飞行中”）。它用于限制对同一缺失区块的重复请求，防止资源浪费。
    * **字段**: `inner: Mutex<HashMap<BlockRef, BTreeSet<AuthorityIndex>>>`。键是缺失区块的引用，值是一个 `BTreeSet`，包含正在尝试获取该区块的 Authority 索引。
    * **`lock_blocks()`**: 尝试为 `missing_block_refs` 中的区块获取锁。如果一个区块已被 `MAX_AUTHORITIES_TO_FETCH_PER_BLOCK` 个 Authority 锁定，或者已经被当前 `peer` 锁定，则不会再次锁定。返回一个 `BlocksGuard`，包含所有成功锁定的区块引用。
    * **`unlock_blocks()`**: 释放指定 `peer` 对 `block_refs` 的锁。
    * **`swap_locks()`**: 尝试原子地将一个 `BlocksGuard` 的锁所有权从当前 `peer` 转移到 `next_peer`（如果成功）。
    * `num_of_locked_blocks()`: 仅用于测试，返回当前被锁定的区块总数。

### 2. `Command` 枚举

* **作用**: 定义了 `Synchronizer` 接收到的各种内部命令。
* **枚举成员**:
    * `FetchBlocks`: 显式请求获取一组缺失的区块。
    * `FetchOwnLastBlock`: 请求获取本节点自己的最后已知区块（用于失忆恢复）。
    * `KickOffScheduler`: 触发周期性同步调度器立即运行。

### 3. `SynchronizerHandle` 结构体

#### 3.1
* **作用**: `Synchronizer` Actor 的公共句柄，允许其他组件向 `Synchronizer` 发送命令。
* **字段**: `commands_sender` (命令发送器), `tasks` (管理内部异步任务的 `JoinSet`)。

#### 3.2 fetch_blocks
```fetch_blocks()```
* **作用**: 向 `Synchronizer` 发送 `FetchBlocks` 命令，请求获取一组缺失区块。
* **逻辑**: 创建一个 `oneshot` 通道用于接收结果，将 `FetchBlocks` 命令发送到 `commands_sender`，并等待 `oneshot` 接收器返回结果。

#### 3.3 stop
```stop()```
* **作用**: 停止 `Synchronizer` 的所有内部任务。
* **逻辑**: 获取 `tasks` 的互斥锁，`abort_all()` 中止所有任务，然后 `join_next()` 等待它们实际结束。

### 4. `Synchronizer` 结构体

* **作用**: `Synchronizer` Actor 的核心实现，包含所有状态和调度逻辑。
* **类型参数 `C`, `V`, `D`**: 分别是 `NetworkClient`, `BlockVerifier`, `CoreThreadDispatcher` 的泛型实现。
* **字段**:
    * `context`, `network_client`, `block_verifier`, `core_dispatcher`, `commit_vote_monitor`, `dag_state`, `transaction_certifier`: 核心依赖。
    * `commands_receiver`: 接收来自 `SynchronizerHandle` 的命令。
    * `fetch_block_senders`: `BTreeMap<AuthorityIndex, Sender<BlocksGuard>>`，每个对等节点都有一个通道，用于向该对等节点的区块获取任务发送 `BlocksGuard`。
    * `fetch_blocks_scheduler_task`: `JoinSet<()>`，管理周期性获取缺失区块的调度任务。
    * `fetch_own_last_block_task`: `JoinSet<()>`，管理获取本节点最后已知区块的任务。
    * `inflight_blocks_map`: `Arc<InflightBlocksMap>`，管理飞行中的区块锁。
    * `commands_sender`: 用于向自身发送命令（例如 `KickOffScheduler`）。

### 5. `impl Synchronizer<C, V, D>` 方法
#### 5.1 start
```pub(crate) fn start(...) -> Arc<SynchronizerHandle>```
* **作用**: `Synchronizer` Actor 的构造和启动函数。
* **逻辑**:
    * 创建 `commands_sender` 和 `commands_receiver`。
    * 创建 `inflight_blocks_map`。
    * **为每个对等节点启动一个 `fetch_blocks_from_authority` 任务**：这个任务负责从特定对等节点获取区块。它通过 `fetch_block_senders` 映射中的通道接收 `BlocksGuard`。
    * 如果 `sync_last_known_own_block` 为 true（表示需要失忆恢复），则向 `commands_sender` 发送 `FetchOwnLastBlock` 命令。
    * **启动主 `run()` 循环任务**: 在一个独立的异步任务中运行 `self.run().await`。
    * 返回一个 `Arc<SynchronizerHandle>`，供外部组件与 `Synchronizer` 交互。

#### 5.2 run
```async fn run(&mut self)```
* **作用**: `Synchronizer` 的主事件循环。它同时监听传入命令、周期性调度器和后台任务的完成。
* **逻辑**:
    * 定义 `PERIODIC_FETCH_INTERVAL` (200ms)。
    * 使用 `tokio::select!` 同时等待：
        1.  **`Some(command) = self.commands_receiver.recv()`**: 接收来自 `SynchronizerHandle` 的命令。
            * `Command::FetchBlocks`: 处理显式获取区块请求。它会尝试为请求的区块获取 `BlocksGuard`，如果成功，则将 `BlocksGuard` 发送到对应对等节点的 `fetch_block_senders` 通道。如果通道已满，则跳过请求，并记录 `SynchronizerSaturated` 错误（表示饱和）。
            * `Command::FetchOwnLastBlock`: 触发 `start_fetch_own_last_block_task()` 来获取本节点的最后已知区块。
            * `Command::KickOffScheduler`: 立即重置周期性调度器，使其尽快运行。
        2.  **`Some(result) = self.fetch_own_last_block_task.join_next()`**: 处理 `start_fetch_own_last_block_task` 任务的完成。
        3.  **`Some(result) = self.fetch_blocks_scheduler_task.join_next()`**: 处理周期性获取缺失区块调度任务的完成。
        4.  **`() = &mut scheduler_timeout`**: 周期性计时器触发。
            * 如果 `fetch_blocks_scheduler_task` 为空（即上次调度任务已完成），则调用 `start_fetch_missing_blocks_task()` 启动一个新的调度任务。
            * 重置计时器以进行下一次周期性触发。

#### 5.3 fetch_blocks_from_authority
```async fn fetch_blocks_from_authority(...)```
    * **作用**: 为**单个特定对等节点** (`peer_index`) 持续获取其持有的缺失区块。这个任务在 `Synchronizer::start` 中被 `spawn`。
    * **逻辑**:
        * 在循环中，等待从 `receiver` 接收 `BlocksGuard`。
        * 一旦收到 `BlocksGuard`，它会调用 `Self::fetch_blocks_request()` 向 `peer_index` 发送实际的网络请求。
        * 如果请求成功，则调用 `Self::process_fetched_blocks()` 处理返回的区块。
        * 如果请求失败（网络错误或超时），并且重试次数未达到 `MAX_RETRIES`，则将相同的 `BlocksGuard` 推回 `requests` 集合进行重试。
        * 如果达到最大重试次数，则放弃该 `BlocksGuard`（通过 `drop(blocks_guard)` 释放锁）。

#### 5.4 process_fetched_blocks
```async fn process_fetched_blocks(...)```
    * **作用**: 处理从对等节点获取到的原始字节数据，将其验证并发送到 Core。
    * **逻辑**:
        * **截断**: 根据配置 (`max_blocks_per_sync`) 截断返回的区块数量。
        * **验证**: 在一个 `spawn_blocking` 任务中并行验证所有获取到的区块（因为验证是 CPU 密集型操作）。
        * **提交投票监控**: 遍历验证后的区块，调用 `commit_vote_monitor.observe_block()`。
        * **指标更新**: 记录 `synchronizer_fetched_blocks_by_peer` 等指标。
        * **添加到 Core**: 调用 `core_dispatcher.add_blocks(blocks).await` 将验证后的区块添加到 Core 的 DAG 中。
        * **解锁**: `drop(requested_blocks_guard)` 释放区块锁。
        * **触发调度器**: 如果 Core 报告仍有缺失区块 (`!missing_blocks.is_empty()`)，则向 `commands_sender` 发送 `KickOffScheduler` 命令，立即触发周期性同步。

#### 5.5 fetch_blocks_request
```async fn fetch_blocks_request(...)```
    * **作用**: 封装向 `NetworkClient` 发送 `fetch_blocks` RPC 请求的逻辑，包括超时和重试。
    * **逻辑**: 调用 `network_client.fetch_blocks()`，并使用 `tokio::time::timeout` 限制请求时间。在超时或网络错误时，递增重试计数，并添加延迟。

#### 5.6 start_fetch_own_last_block_task
```fn start_fetch_own_last_block_task(&mut self)```
    * **作用**: 启动一个异步任务，负责从其他对等节点获取本节点最后已知提议的区块（用于失忆恢复）。
    * **逻辑**:
        * 在循环中，它会向委员会中的所有其他对等节点发送 `fetch_latest_blocks` 请求，以获取本节点自己的最新区块。
        * **聚合结果**: 收集来自其他节点的响应，找到其中最高的轮次。
        * **法定数量检查**: 持续重试，直到从足够数量（达到 `validity` 门限，通常是 `f+1`）的对等节点收集到结果，确保得到一个可靠的最高轮次。
        * **更新 Core**: 获取到最高轮次后，更新 `context.metrics.node_metrics.last_known_own_block_round` 指标，并调用 `core_dispatcher.set_last_known_proposed_round()` 通知 Core 模块。
        * 包含指数退避重试逻辑。

#### 5.7 start_fetch_missing_blocks_task
```async fn start_fetch_missing_blocks_task(&mut self) -> ConsensusResult<()>```
    * **作用**: 启动周期性获取缺失区块的主调度任务。
    * **逻辑**:
        * 首先从 `core_dispatcher.get_missing_blocks()` 获取当前缺失的区块列表。
        * 如果缺失区块为空，则直接返回。
        * **跳过提交滞后状态**: `if self.is_commit_lagging()`: 如果当前节点处于提交滞后状态，则直接返回，不进行周期性获取。这是因为此时节点正在通过 `commit_syncer` 进行大量同步，块同步可能被认为是低优先级或冗余的。
        * `fetch_blocks_scheduler_task.spawn(...)`: 启动一个异步任务 `fetch_blocks_from_authorities` 或 `fetch_blocks_from_authorities_old`，负责实际从其他节点获取缺失区块。

#### 5.8 is_commit_lagging
```fn is_commit_lagging(&self) -> bool```
    * **作用**: 判断节点是否处于“提交滞后”状态。
    * **逻辑**: 比较 `last_commit_index` (本节点最后提交的索引) 和 `quorum_commit_index` (网络中仲裁已提交的索引)。如果本节点滞后 `commit_sync_batch_size * COMMIT_LAG_MULTIPLIER` 的数量，则认为处于滞后状态。


### 总结：

`Synchronizer` 确保节点的 DAG 状态始终与网络保持同步。它通过：

* **多层同步机制**：结合显式请求（针对新收到的区块的祖先）和周期性请求（针对所有未解决的缺失区块）。
* **智能的资源管理**：通过 `InflightBlocksMap` 避免重复获取同一区块，并限制并发请求数量。
* **鲁棒的重试策略**：内置指数退避和超时机制来处理网络不稳定和对等节点故障。
* **失忆恢复支持**：通过 `FetchOwnLastBlock` 任务协助节点从数据丢失中恢复。
* **可观测性**：广泛使用日志和指标来跟踪同步过程的各个阶段和性能。

这个模块对于维持 Sui 网络的健康和节点的一致性至关重要。

---
./core/src/proposed_block_handler.rs
---
### `ProposedBlockHandler` 模块的职责：

`ProposedBlockHandler` 的主要职责是**处理本节点提议的新区块的后续异步逻辑**。它监听共识核心广播的提议区块，然后执行一些与这些区块相关的操作，例如将它们添加到事务认证器中。

### 1. `ProposedBlockHandler` 结构体

* **作用**: 封装了提议区块处理器的内部状态。
* **字段**:
    * `context: Arc<Context>`: 节点的全局上下文，提供协议配置等共享信息。
    * `rx_block_broadcast: broadcast::Receiver<ExtendedBlock>`: 一个广播接收器，用于接收来自共识核心的**本节点提议的**新区块。这是 `CoreSignals::new()` 创建的通道中的一个接收端。
    * `transaction_certifier: TransactionCertifier`: 事务认证器，`ProposedBlockHandler` 会将提议的区块传递给它进行处理。

### 2. `impl ProposedBlockHandler` 方法
#### 2.1 run
`pub(crate) async fn run(&mut self)`
* **作用**: 这是 `ProposedBlockHandler` 的核心异步任务。它在一个无限循环中运行，持续监听并处理本节点提议的新区块。
* **逻辑**:
    * `loop { ... }`: 表示这是一个无限循环，任务会持续运行。
    * `match self.rx_block_broadcast.recv().await { ... }`: 等待从 `rx_block_broadcast` 通道接收下一个 `ExtendedBlock`。
        `Ok(extended_block)` 成功接收到一个提议区块。然后调用 `self.handle_proposed_block(extended_block)` 方法来处理这个区块。
        `Err(broadcast::error::RecvError::Closed)` 如果广播通道被关闭（通常意味着共识核心正在关闭），则记录 `trace` 日志并返回，任务结束。
        `Err(broadcast::error::RecvError::Lagged(e))` 如果接收器滞后（即无法及时处理所有发送的区块，导致一些区块被跳过），记录 `warn` 日志并继续循环，尝试接收下一个区块。这表明 `ProposedBlockHandler` 具有一定的容错能力，可以处理消息积压的情况。

#### 2.2 handle_proposed_block
`fn handle_proposed_block(&self, extended_block: ExtendedBlock)`
* **作用**: 处理单个提议的区块。
* **参数**: `extended_block`，包含本节点提议的区块及其扩展信息。
* **逻辑**:
    * `if !self.context.protocol_config.mysticeti_fastpath() { return; }`: 这是一个条件检查。只有当协议配置中启用了 `mysticeti_fastpath`（Sui 的快速路径共识模式）时，才会执行后续的逻辑。如果未启用，则直接返回，不处理提议区块。
    * `let _scope = monitored_scope("handle_proposed_block");`: 用于度量和追踪，在日志和指标中标记这个函数的执行范围。
    * **垃圾回收**: `self.transaction_certifier.run_gc();`: 调用事务认证器的垃圾回收方法。这可能是在处理新区块之前，清理掉一些不再需要的旧数据或状态。
    * **添加到事务认证器**: `self.transaction_certifier.add_proposed_block(extended_block.block.clone());`: 将提议的区块添加到 `transaction_certifier` 中。事务认证器可能会对这些区块中的事务进行处理，例如跟踪它们的状态、进行认证等。

### 总结：

`ProposedBlockHandler` 是 Sui 共识模块中处理本节点所提议区块的**后台任务**。
它确保在区块被提议并广播出去后，相关的后续逻辑（如事务认证、垃圾回收等）能够被及时执行。它的核心作用是：

1.  **分发提议区块**: 从共识核心接收本节点提议的区块。
2.  **触发事务认证**: 将提议的区块传递给 `TransactionCertifier`，这是共识核心与事务处理之间的桥梁。
3.  **支持快速路径**: 仅在 `mysticeti_fastpath` 启用时才执行其核心逻辑。
4.  **健壮性**: 具有对广播通道关闭和滞后情况的容错处理能力。

这个组件是确保共识协议中领导者节点（本节点）提议的区块能够被有效处理，并与其他共识子系统正确交互的关键部分。


---
./core/src/round_prober.rs
---

### `RoundProber` 模块的职责：

`RoundProber` 的主要职责是**周期性地探测网络中的其他对等节点，获取它们“收到”和“接受”的最新轮次信息**。通过收集这些信息，`RoundProber` 可以：

1.  **评估区块传播效率**: 了解本节点提议的区块在网络中传播和被接受的速度。
2.  **判断网络健康状况**: 即使在没有区块提议时也能保持活跃，从而帮助判断网络的活跃度。
3.  **支持优化决策**: 根据探测到的传播延迟，帮助系统决定何时启用或禁用可能影响活性的优化措施。

### 1. `RoundProberHandle` 结构体

* **作用**: `RoundProber` Actor 的公共句柄，用于外部与 `RoundProber` 任务进行交互。
* **字段**:
    * `prober_task: JoinHandle<()>`: 异步任务的句柄，代表 `RoundProber` 的主循环。
    * `shutdown_notify: Arc<NotifyOnce>`: 一个原子通知器，用于向 `RoundProber` 任务发送关机信号。
* **`pub(crate) async fn stop(self)`**:
    * **作用**: 停止 `RoundProber` 的主循环任务。
    * **逻辑**: 调用 `self.shutdown_notify.notify()` 发送关闭信号，然后 `await` `self.prober_task`，等待任务正常退出。它不会强制中止任务（`abort()`），以允许任务在关闭前完成清理。如果任务 panic，则恢复 panic。

### 2. `RoundProber` 结构体

* **作用**: `RoundProber` Actor 的核心实现，包含其所有状态和逻辑。
* **类型参数 `C: NetworkClient`**: `C` 必须实现 `NetworkClient` trait，用于向其他对等节点发送网络请求。
* **字段**:
    * `context: Arc<Context>`: 节点的全局上下文。
    * `core_thread_dispatcher: Arc<dyn CoreThreadDispatcher>`: 与共识核心线程通信的调度器，用于获取其 `highest_received_rounds` 和设置 `propagation_delay`。
    * `round_tracker: Arc<RwLock<PeerRoundTracker>>`: 跟踪对等节点轮次信息的组件，`RoundProber` 会更新其中的数据。
    * `dag_state: Arc<RwLock<DagState>>`: DAG 状态的读写锁包装，用于获取本节点缓存的最新区块信息。
    * `network_client: Arc<C>`: 网络客户端的共享引用，用于实际发送 `get_latest_rounds` 请求。
    * `shutdown_notify: Arc<NotifyOnce>`: 用于接收外部停止信号的通知器。

### 3. `impl RoundProber<C>` 方法
#### 3.1 start

`pub(crate) fn start(self) -> RoundProberHandle`
* **作用**: 启动 `RoundProber` 的主循环任务，并返回一个 `RoundProberHandle` 句柄，供外部控制。
* **逻辑**:
    * 克隆 `shutdown_notify`，一个用于任务内部，一个用于返回给句柄。
    * `tokio::spawn(async move { ... })`: 在一个独立的 Tokio 异步任务中启动 `RoundProber` 的主循环。
    * **周期性定时器**: `let mut interval = tokio::time::interval(Duration::from_millis(self.context.parameters.round_prober_interval_ms));`：设置一个周期性计时器，以 `round_prober_interval_ms` (默认 200ms) 为间隔触发。`set_missed_tick_behavior(MissedTickBehavior::Delay)` 确保如果任务处理时间超过间隔，下一个 tick 会立即出现，而不是跳过。
    * **主循环 (`loop`)**: 使用 `tokio::select!` 同时等待两个事件：
        1.  **`_ = interval.tick()` 当周期性计时器触发时，调用 `self.probe().await` 执行探测逻辑。
        2.  **`_ = loop_shutdown_notify.wait()` 等待关闭通知。一旦接收到通知，`break` 跳出循环，任务结束。

#### 3.2 probe
`pub(crate) async fn probe(&self) -> Round`
* **作用**: 执行一次周期性的对等节点轮次探测。这是 `RoundProber` 的核心工作。
* **逻辑**:
    * **度量范围**: `let _scope = monitored_scope("RoundProber");` 用于在日志和指标中标记此函数的执行范围。
    * **初始化数据结构**:
        * `highest_received_rounds`: 二维向量，存储每个对等节点报告的，它接收到的其他所有对等节点的最高轮次。
        * `highest_accepted_rounds`: 类似，存储每个对等节点报告的，它接受的其他所有对等节点的最高轮次。
    * **请求超时**: `request_timeout` 从 `context.parameters` 获取。
    * **并发请求**: `let mut requests = FuturesUnordered::new();`：为除了本节点之外的每个对等节点，启动一个异步任务来调用 `network_client.get_latest_rounds()` 方法，并将其添加到 `requests` 集合中并发执行。每个请求都有超时限制。
    * **处理响应**: `loop { tokio::select! { ... } }`:
        `result = requests.next()` 当某个请求完成时：
            * **成功 (`Ok(Ok(...))`)**: 如果成功获取到 `(received, accepted)` 轮次列表，则将其存储到 `highest_received_rounds` 和 `highest_accepted_rounds` 中对应对等节点的位置。
            * **网络错误 (`Ok(Err(err))`)**: 记录 debug 日志和 `round_prober_request_errors` 指标，表示获取失败。
            * **超时 (`Err(_)`)**: 记录 debug 日志和 `round_prober_request_errors` 指标，表示请求超时。
        `_ = self.shutdown_notify.wait()` 如果在所有请求完成之前收到关闭信号，则提前退出循环。
    * **更新 `RoundTracker`
        * `self.round_tracker.write().update_from_probe(...)`: 将收集到的所有对等节点的最高接收轮次和最高接受轮次信息更新到 `PeerRoundTracker` 中。
        * `PeerRoundTracker` 会根据这些数据计算网络的传播延迟。
    * **计算传播延迟**: `let propagation_delay = self.round_tracker.read().calculate_propagation_delay(last_proposed_round);` 计算本节点区块的传播延迟。
    * **通知 Core**: `let _ = self.core_thread_dispatcher.set_propagation_delay(propagation_delay);` 将计算出的传播延迟发送给共识核心线程。
    * **返回**: 返回计算出的传播延迟。

### 总结：

`RoundProber` 是 Sui 共识模块中一个重要的**网络监控和自适应优化组件**。它通过：

* **周期性探测**: 定期向所有对等节点发送请求，收集它们的最新轮次信息。
* **全面数据收集**: 收集每个对等节点对所有其他对等节点的接收和接受轮次视图。
* **传播延迟计算**: 利用收集到的数据计算本节点提议区块的传播延迟。
* **反馈给 Core**: 将传播延迟信息反馈给共识核心，以便核心可以根据网络状况调整其行为（例如，是否启用某些依赖于低延迟或高传播效率的优化）。
* **鲁棒性**: 具有对网络错误和超时的处理机制，并能容忍部分请求失败。
* **资源清理**: `stop()` 方法确保任务的优雅关闭。



---
./core/src/block_manager.rs
---

### `BlockManager` 模块的职责：

`BlockManager` 的核心职责是**管理节点的内存中区块图（DAG）的状态**，特别是处理**区块的接收、验证、悬挂（suspend）、解除悬挂（unsuspend）以及识别缺失祖先区块**。它充当了共识核心与底层存储之间的一个中间层，确保区块能够被正确地连接到 DAG 中。

### 1. `SuspendedBlock` 结构体

* **作用**: 表示一个暂时无法被立即处理的区块。
* **字段**:
    * `block: VerifiedBlock`: 实际被悬挂的区块。
    * `missing_ancestors: BTreeSet<BlockRef>`: 该区块**缺失的祖先区块**的引用集合。只有当这些祖先区块都被处理后，该区块才能被解除悬挂。
    * `timestamp: Instant`: 记录区块被悬挂的时间，可能用于度量或清理。

### 2. `BlocksGuard` 结构体

* **作用**: 一个 RAII（Resource Acquisition Is Initialization）风格的守卫，用于管理对 `InflightBlocksMap` 中区块锁的生命周期。
* **`Drop` 实现**: 当 `BlocksGuard` 实例离开作用域时，会自动调用 `map.unlock_blocks()` 方法，释放其持有的区块锁。这确保了即使请求失败或提前退出，区块锁也能被正确释放。

### 3. `InflightBlocksMap` 结构体

* **作用**: 管理“飞行中”的区块锁，用于限制对同一缺失区块的重复请求。
* **字段**: `inner: Mutex<HashMap<BlockRef, BTreeSet<AuthorityIndex>>>`。键是缺失区块的引用，值是一个 `BTreeSet`，包含正在尝试获取该区块的 Authority 索引。

#### 3.1 lock_blocks
`lock_blocks()`
* **作用**: 尝试为一组 `missing_block_refs` 中的区块获取锁。
* **逻辑**: 遍历传入的 `missing_block_refs`。对于每个区块，检查当前正在获取它的 Authority 数量是否小于 `MAX_AUTHORITIES_TO_FETCH_PER_BLOCK` (默认为 2)，并且当前请求的 `peer` 没有重复锁定它。如果满足条件，则将 `peer` 添加到该区块的 Authority 集合中，并将该区块添加到待返回的 `blocks` 集合中。
* 返回一个 `Option<BlocksGuard>`，表示成功锁定的区块。

#### 3.2 unlock_blocks
`unlock_blocks()`
* **作用**: 释放指定 `peer` 对 `block_refs` 的锁。
* **逻辑**: 遍历 `block_refs`，从 `inner` 映射中找到对应的区块条目，并移除 `peer`。如果移除后，该区块不再被任何 Authority 锁定，则从 `inner` 中删除该区块。

#### 3.3 swap_locks
`swap_locks()`
* **作用**: 尝试将一个 `BlocksGuard` 的所有权从当前 `peer` 转移到 `next_peer`。
* **逻辑**: 先 `drop(blocks_guard)` 释放当前 `peer` 的锁，然后尝试为 `next_peer` 获取新锁。

### 4. `BlockManager` 结构体

* **作用**: 核心的区块管理逻辑，负责 DAG 状态的更新和区块的生命周期管理。
* **字段**:
    * `context: Arc<Context>`: 节点的全局上下文。
    * `dag_state: Arc<RwLock<DagState>>`: DAG 状态的读写锁，保存已接受区块的 DAG。
    * `block_verifier: Arc<dyn BlockVerifier>`: 区块验证器，用于验证区块。
    * `suspended_blocks: BTreeMap<BlockRef, SuspendedBlock>`: 存储所有被悬挂的区块（等待祖先）。
    * `missing_ancestors: BTreeMap<BlockRef, BTreeSet<BlockRef>>`: 存储缺失祖先（键）和依赖这些缺失祖先的子区块（值）的映射。
    * `missing_blocks: BTreeSet<BlockRef>`: 存储实际缺失且尚未获取的区块引用（即不在 `dag_state` 和 `suspended_blocks` 中的）。
    * `received_block_rounds: Vec<Option<(Round, Round)>>`: 跟踪每个 Authority 接收到的区块的最低和最高轮次（用于度量）。

### 5. `impl BlockManager` 方法
#### 5.1 try_accept_blocks
`pub(crate) fn try_accept_blocks(...) -> (Vec<VerifiedBlock>, BTreeSet<BlockRef>)`
* **作用**: 尝试接受传入的区块列表。这是接收新区块后的**主要入口**。
* **逻辑**: 调用 `try_accept_blocks_internal(blocks, false)`。

#### 5.2 try_accept_committed_blocks
`pub(crate) fn try_accept_committed_blocks(...) -> Vec<VerifiedBlock>`
* **作用**: 尝试接受已提交的区块。这些区块被认为是**权威的**，不应有缺失祖先。
* **逻辑**: 调用 `try_accept_blocks_internal(blocks, true)`，并断言没有返回缺失区块。

#### 5.3 try_accept_blocks_internal
`fn try_accept_blocks_internal(...) -> (Vec<VerifiedBlock>, BTreeSet<BlockRef>)`
* **作用**: 核心的区块接受逻辑。
* **参数**: `blocks` (待处理区块), `committed` (是否为已提交区块)。
* **逻辑**:
    * 对输入区块按轮次排序。
    * 遍历每个区块：
        * `self.update_block_received_metrics()`: 更新接收区块的度量。
        `self.try_accept_one_committed_block()` (如果 `committed == true`)**: 处理已提交区块。
            * 如果区块已在 `dag_state` 中，跳过。
            * 如果区块曾被悬挂，则从 `suspended_blocks` 和 `missing_ancestors` 中移除。
            * 将区块直接添加到 `dag_state`。
        `self.try_accept_one_block()` (如果 `committed == false`)**: 处理普通区块。
            * 如果区块已在 `suspended_blocks` 或 `dag_state` 中，跳过。
            * 如果区块轮次小于或等于 `gc_round`，则**跳过**处理 (`TryAcceptResult::Skipped`)。
            * **检查祖先**: 遍历区块的祖先，如果祖先不在 `dag_state` 中，则标记为 `missing_ancestors`，并可能将其添加到 `self.missing_ancestors` 和 `self.missing_blocks` 中。
            * 如果有缺失祖先，则将当前区块作为 `SuspendedBlock` 存储，并返回 `missing_ancestors` 列表。
            * 如果没有缺失祖先，则返回 `TryAcceptResult::Accepted(block)`。
        * **解除悬挂子区块**: 如果当前区块被接受，调用 `self.try_unsuspend_children_blocks()` 尝试解除其子区块的悬挂。
        * **时间戳验证和接受**: 调用 `self.verify_block_timestamps_and_accept()` 验证区块时间戳，并将有效区块添加到 `dag_state`。
    * 返回所有成功接受的区块和新发现的缺失祖先区块。

#### 5.4 try_find_blocks
`pub(crate) fn try_find_blocks(...) -> BTreeSet<BlockRef>`
* **作用**: 尝试查找一组 `BlockRef` 是否存在于 `dag_state` 或 `BlockManager` 的 `suspended_blocks` 中，并返回仍然缺失的区块引用集合。
* **逻辑**:
    * 过滤掉轮次小于或等于 `gc_round` 的区块引用。
    * 遍历剩余的区块引用，检查它们是否在 `dag_state` 或 `suspended_blocks` 中。
    * 如果不在，则将其添加到 `missing_blocks` 集合中，并可能更新 `missing_ancestors`。
    * 更新相关度量。
    * 返回所有被认为是缺失的区块引用。

#### 5.5 verify_block_timestamps_and_accept
`fn verify_block_timestamps_and_accept(...) -> Vec<VerifiedBlock>`
* **作用**: 验证区块的时间戳，并将通过验证的区块接受到 `dag_state`。
* **逻辑**:
    * 如果协议配置使用中位数时间戳，则跳过时间戳验证。
    * 否则，调用 `self.verify_block_timestamps()` 进行详细的时间戳验证。
    * 将通过验证的区块批量插入到 `dag_state` 中。

#### 5.6 verify_block_timestamps
`fn verify_block_timestamps(...) -> Vec<VerifiedBlock>`
* **作用**: （旧版或非中位数时间戳模式）基于祖先区块的时间戳，验证区块的时间戳是否有效。
* **逻辑**:
    * 遍历区块，获取其祖先区块。
    * 调用 `block_verifier.check_ancestors()` 检查区块时间戳是否在其祖先区块的时间戳之后，且不在未来太远。
    * 如果验证失败，将区块标记为拒绝。
    * 最终返回通过验证的区块。

#### 5.7 try_unsuspend_children_blocks
`fn try_unsuspend_children_blocks(...) -> Vec<VerifiedBlock>`
* **作用**: 当一个区块 (`accepted_block`) 被接受后，尝试解除其所有直接子区块的悬挂状态。
* **逻辑**:
    * 使用一个队列进行广度优先遍历。
    * 遍历 `missing_ancestors`，找到依赖 `accepted_block` 的子区块。
    * 对于每个这样的子区块，调用 `self.try_unsuspend_block()` 尝试解除其悬挂。
    * 如果成功解除悬挂，则将该子区块添加到 `unsuspended_blocks` 列表和待处理队列中，以便递归检查其子区块。
    * 记录解除悬挂的区块数量和在悬挂队列中花费的时间。

#### 5.8 try_unsuspend_block
`fn try_unsuspend_block(...) -> Option<SuspendedBlock>`
* **作用**: 尝试解除单个区块的悬挂状态。
* **逻辑**:
    * 从 `suspended_blocks` 中获取区块。
    * 从该区块的 `missing_ancestors` 集合中移除已接受的依赖 `accepted_dependency`。
    * 如果移除后 `missing_ancestors` 为空，说明该区块不再有缺失依赖，可以被解除悬挂，并从 `suspended_blocks` 中移除并返回。

#### 5.9 try_unsuspend_blocks_for_latest_gc_round
`pub(crate) fn try_unsuspend_blocks_for_latest_gc_round(&mut self)`
* **作用**: 尝试解除那些由于 GC（垃圾回收）轮次推进而不再缺少祖先区块的悬挂区块。
* **逻辑**:
    * 获取当前的 `gc_round`。
    * 遍历 `missing_ancestors`（按键排序）。如果某个缺失祖先的轮次小于或等于 `gc_round`，那么它就不再是真正的缺失（因为它已经被 GC 掉，不再需要获取）。
    * 从 `missing_ancestors` 和 `missing_blocks` 中移除这些被 GC 的祖先。
    * 然后，对依赖这些被 GC 祖先的子区块调用 `self.try_unsuspend_children_blocks()` 尝试解除悬挂。
    * 验证并通过时间戳检查后，接受这些区块到 `dag_state`。

#### 5.10 missing_blocks
`pub(crate) fn missing_blocks(&self) -> BTreeSet<BlockRef>`
返回当前所有被 `BlockManager` 记录为缺失的区块引用集合。

#### 5.11 私有辅助方法
`update_stats()` 和 `update_block_received_metrics()` 
用于更新各种 Prometheus 指标，如悬挂区块数量、缺失祖先数量、接收区块的轮次范围等。


### 总结：

`BlockManager` 是 Sui 共识模块中处理区块生命周期和 DAG 连通性的核心组件：

* **暂存区块**: 智能地管理因缺失祖先而无法立即处理的区块。
* **高效识别缺失**: 快速识别并跟踪区块的缺失祖先。
* **解除依赖**: 在祖先区块到达后，自动解除其子区块的悬挂。
* **时间戳验证**: 在将区块完全接受到 DAG 前，进行时间戳的有效性验证。
* **应对 GC**: 处理因 GC 推进而导致的旧区块依赖问题。
* **防止 OOM**: 虽代码中 `TODO` 提到，但其内部锁机制和对并发请求的限制也间接有助于防止内存耗尽。

这个模块对于维护 Sui 共识协议的 DAG 状态的正确性、避免数据损坏和确保节点能够从部分数据缺失中恢复至关重要。

---
./core/src/commit_syncer.rs
---

### `CommitSyncer` 模块的职责：

`CommitSyncer` 的核心职责是**高效地同步已提交的共识数据（commits 和它们引用的区块）**。当一个验证者节点落后于网络的共识进度时（例如由于网络中断或崩溃恢复），`CommitSyncer` 会负责从其他对等节点获取这些缺失的提交，从而帮助节点追赶到最新状态。

文档明确指出，`CommitSyncer` 这样做有几个关键点：

1.  **信任已提交数据**: 依赖于“法定数量 (`>= 2f+1` ) 验证者认证的提交是可信的”这一原则，因此不需要对这些提交中的区块进行完全的重新验证（但它们仍然会通过 Core 处理）。
2.  **并行抓取**: 利用提交的线性依赖关系，支持并行抓取提交范围，提高同步效率。
3.  **非关键路径操作**: 提交同步是一个开销较大的操作，但它不在区块处理的关键路径上，因此在触发和重试策略上会倾向于吞吐量和资源效率，而非快速反应。

### 1. `CommitSyncerHandle` 结构体

* **作用**: `CommitSyncer` Actor 的公共句柄，用于外部组件控制 `CommitSyncer` 的生命周期。
* **字段**:
    * `schedule_task: JoinHandle<()>`: 异步任务的句柄，代表 `CommitSyncer` 的主调度循环。
    * `tx_shutdown: oneshot::Sender<()>`: 一个 `oneshot` 发送器，用于向 `CommitSyncer` 任务发送关闭信号。
* **`pub(crate) async fn stop(self)`**:
    * **作用**: 停止 `CommitSyncer` 的主循环任务。
    * **逻辑**: 发送关闭信号 (`tx_shutdown.send(())`)，然后等待 `schedule_task` 优雅退出。它不会强制中止任务（`abort()`），以允许任务在关闭前完成清理。

### 2. `CommitSyncer` 结构体

* **作用**: `CommitSyncer` Actor 的核心实现，包含其所有状态和调度逻辑。
* **类型参数 `C: NetworkClient`**: `C` 必须实现 `NetworkClient` trait，用于向其他对等节点发送网络请求（特别是 `fetch_commits` 和 `fetch_blocks`）。
* **关键字段**:
    * `inner: Arc<Inner<C>>`: 共享的内部状态包装器，包含了 `CommitSyncer` 需要的所有核心依赖（Context, CoreThreadDispatcher, NetworkClient, etc.）。
    * `inflight_fetches: JoinSet<(u32, CertifiedCommits)>`: 正在进行的提交抓取请求的集合。元组中的 `u32` 可能是目标结束提交索引，`CertifiedCommits` 是抓取到的提交数据。
    * `pending_fetches: BTreeSet<CommitRange>`: 尚未开始抓取但已安排的提交范围集合。
    * `fetched_ranges: BTreeMap<CommitRange, CertifiedCommits>`: 已抓取并等待 Core 处理的提交范围和对应数据。
    * `highest_scheduled_index: Option<CommitIndex>`: 已调度抓取的最高提交索引。
    * `highest_fetched_commit_index: CommitIndex`: 成功抓取到的最高提交索引（经过验证）。
    * `synced_commit_index: CommitIndex`: 已经发送给 Core 并处理的最高提交索引。

### 3. `impl CommitSyncer<C>` 方法
#### 3.1 new
`pub(crate) fn new(...) -> Self`
* **作用**: `CommitSyncer` 的构造函数。
* **逻辑**: 初始化所有内部字段，特别是创建 `Inner` 实例并将其包装在 `Arc` 中。`synced_commit_index` 被初始化为当前节点本地存储的最后提交索引。

#### 3.2 start
`pub(crate) fn start(self) -> CommitSyncerHandle`
* **作用**: 启动 `CommitSyncer` 的主调度循环，并返回 `CommitSyncerHandle`。
* **逻辑**: 创建一个 `oneshot` 通道用于关闭信号，然后在独立的 Tokio 任务中启动 `self.schedule_loop()`。

#### 3.3 schedule_loop
`async fn schedule_loop(mut self, mut rx_shutdown: oneshot::Receiver<()>)`
* **作用**: `CommitSyncer` 的主调度循环，持续检查是否需要调度新的抓取任务。
* **逻辑**:
    * **周期性调度**: 使用 `tokio::time::interval(Duration::from_secs(2))` 设置一个 2 秒的周期性计时器，定期触发调度检查。`set_missed_tick_behavior(MissedTickBehavior::Skip)` 确保即使任务滞后，也不会积累 `tick`。
    `tokio::select!` 同时等待：
        1.  **计时器触发 (`_ = interval.tick()`)**: 触发 `self.try_schedule_once()` 方法，尝试调度新的抓取任务。
        2.  **抓取任务完成 (`Some(result) = self.inflight_fetches.join_next()`)**: 处理已完成的抓取任务的结果。如果任务 panic 或被取消，则关闭 `CommitSyncer`。如果成功，则调用 `self.handle_fetch_result()` 处理抓取到的提交数据。
        3.  **关机信号 (`_ = &mut rx_shutdown`)**: 收到关闭信号，停止所有正在进行的抓取任务，然后退出循环。
    `self.try_start_fetches()` 每次循环都会尝试启动新的抓取任务，直到达到并行限制或没有待处理的批次。

#### 3.4 try_schedule_once
`fn try_schedule_once(&mut self)`
* **作用**: 尝试根据当前网络的共识进度，调度新的提交抓取任务。
* **逻辑**:
    * 获取 `quorum_commit_index` (网络中法定提交的最高索引) 和 `local_commit_index` (本节点本地的最高提交索引)。
    * 更新 `metrics` 中的相关提交索引。
    * **确定抓取范围**: 从 `fetch_after_index` (上次调度或本地已提交的最高索引) 开始，直到 `quorum_commit_index`，按 `commit_sync_batch_size` 步长生成新的 `CommitRange`。
    * **暂停调度条件**: 如果 `highest_handled_index + unhandled_commits_threshold < range_end`，表示提交处理逻辑（`CommitConsumer`）滞后过多，则暂停调度新的抓取，避免雪崩效应。
    * 将新生成的 `CommitRange` 插入到 `pending_fetches` 集合中。
    * 更新 `highest_scheduled_index`。

#### 3.5 handle_fetch_result
`async fn handle_fetch_result(&mut self, target_end: CommitIndex, certified_commits: CertifiedCommits)`
* **作用**: 处理单个抓取任务（`fetch_loop`）完成后的结果，即从对等节点成功获取到一批 `CertifiedCommits` 数据。
* **逻辑**:
    * **更新度量**: 记录抓取到的提交和区块数量及字节大小。
    * 更新 `highest_fetched_commit_index`。
    * **处理部分结果**: 如果实际抓取到的提交范围小于预期 (`commit_end < target_end`)，则将剩余部分重新加入 `pending_fetches`。
    * **添加已抓取范围**: 如果有新的、未同步的提交被抓取到，将其添加到 `fetched_ranges` 映射中。
    * **发送给 Core 处理**:
        * 循环检查 `fetched_ranges` 中最早的提交范围。
        * **检查间隙**: 如果当前范围的开始索引 `fetched_commit_range.start()` 不等于 `self.synced_commit_index + 1`，说明存在间隙，暂停发送给 Core，等待间隙被填补。
        * 调用 `self.inner.core_thread_dispatcher.add_certified_commits(commits).await` 将 `CertifiedCommits` 发送给共识核心线程处理。
        * 如果核心处理失败，记录错误并退出（这表示致命错误）。
        * **处理缺失祖先**: `add_certified_commits` 可能会返回缺失的祖先区块引用。如果存在，记录警告，但不会重新抓取（因为这些区块通常会在未来的提交中被包含进来）。
        * **推进 `synced_commit_index` 一旦 `CertifiedCommits` 被发送给 Core，更新 `synced_commit_index` 到已处理范围的结束。

#### 3.6 try_start_fetches
`fn try_start_fetches(&mut self)`
* **作用**: 尝试启动新的提交抓取任务，直到达到并行限制或没有待处理的批次。
* **逻辑**:
    * **计算目标并行抓取数**: `target_parallel_fetches` 由 `commit_sync_parallel_fetches`、委员会大小和 `fetched_ranges` 的大小共同决定。
    * **循环启动**: 只要还有待处理的提交范围 (`pending_fetches`) 且未达到并行抓取限制，就从 `pending_fetches` 中弹出一个范围。
    * 为每个弹出的范围，启动一个 `Self::fetch_loop()` 异步任务，并将其添加到 `inflight_fetches` 集合中。
    * 更新 `metrics` 中的 `inflight_fetches` 和 `pending_fetches` 数量。

#### 3.7 fetch_loop
`async fn fetch_loop(inner: Arc<Inner<C>>, commit_range: CommitRange) -> (CommitIndex, CertifiedCommits)`
* **作用**: 这是一个独立的异步任务，负责**反复重试**从对等节点获取一个特定的 `CommitRange`，直到成功。
* **逻辑**:
    * 定义重试超时和最大重试目标。
    * **循环重试**: `loop { ... }`
        * 随机选择 `MAX_NUM_TARGETS` 个对等节点进行尝试。
        * **动态超时**: `request_timeout = TIMEOUT * timeout_multiplier`，每次重试都会增加超时乘数，直到达到 `MAX_TIMEOUT_MULTIPLIER`，从而给予请求更多时间。
        `fetch_once()` 调用**: 为每个目标 Authority，调用 `Self::fetch_once()` 尝试获取提交。
        * **结果处理**:
            * **成功 (`Ok(Ok(commits))`)**: 打印成功日志，并返回抓取到的 `CertifiedCommits`。
            * **网络错误 (`Ok(Err(e))`)**: 记录警告和 `commit_sync_fetch_once_errors` 指标。
            * **超时 (`Err(_)`)**: 记录警告和 `commit_sync_fetch_once_errors` 指标。
        * **休眠**: 如果所有尝试都失败，则休眠 `TIMEOUT` 时长，避免忙循环。

#### 3.8 fetch_once
`async fn fetch_once(inner: Arc<Inner<C>>, target_authority: AuthorityIndex, commit_range: CommitRange, timeout: Duration) -> ConsensusResult<CertifiedCommits>`
* **作用**: 从**单个对等节点** (`target_authority`) 执行一次完整的提交抓取操作，包括获取提交本身和它们引用的区块。
* **逻辑**:
    * **1. 获取提交**: 调用 `inner.network_client.fetch_commits()` 获取指定范围内的序列化提交和认证区块。
    * **2. 验证提交**: 在 `spawn_blocking` 任务中调用 `inner.verify_commits()`（CPU 密集型操作）来验证提交的链式关系和法定投票，以及投票区块的有效性。
    * **3. 获取区块引用**: 从已验证的提交中收集所有引用的 `BlockRef`。
    * **4. 分块请求区块**: 将这些 `BlockRef` 分块，并使用 `FuturesOrdered` **并行地**向同一个 `target_authority` 发送 `fetch_blocks` 请求，以避免一次请求过多数据。在发送这些请求时，还会添加小的延迟（`sleep(timeout * i as u32 / num_chunks).await`），实现请求的流水线化。
    * **5. 验证返回区块**: 验证 `fetch_blocks` 返回的区块数量是否与请求数量匹配，格式是否有效，以及是否与请求的 `BlockRef` 匹配。
    * **6. 时间戳检查**: 检查获取到的区块的时间戳是否在未来太远。
    * **7. 构建 `CertifiedCommits` 将验证后的区块分配给对应的提交，构建 `CertifiedCommits` 对象。
    * **8. 添加到事务认证器**: 如果启用了 `mysticeti_fastpath`，将这些区块添加到 `transaction_certifier` 中。

#### 3.9 verify_commits
`fn verify_commits(...)` (在 `Inner` impl 块中)**:
* **作用**: 验证从对等节点获取到的序列化提交和投票区块的正确性。
* **逻辑**:
    * **反序列化和链式验证**: 遍历序列化提交，反序列化它们，并验证提交索引的连续性、哈希链的正确性。
    * **投票区块验证**: 反序列化投票区块，验证其签名，并聚合投票，确保最后的提交达到法定投票门限。
    * **事务认证器集成**: 如果启用了 `mysticeti_fastpath`，将投票区块添加到 `transaction_certifier` 中。
    * 返回验证后的 `TrustedCommit` 和 `VerifiedBlock` 列表。

### 总结：

`CommitSyncer` 是 Sui 区块链共识模块中**关键的同步机制**。它通过一套复杂的、多阶段的异步流程来高效地追赶网络共识进度：

* **分阶段同步**: 先抓取提交，再抓取提交引用的区块。
* **并行化和负载均衡**: 利用 `FuturesOrdered` 和 `JoinSet` 并发地向多个对等节点发送请求，并将请求分散到不同的对等节点。
* **智能重试和退避**: 在抓取失败时，通过指数退避和超时机制进行重试，并能切换到其他对等节点。
* **资源限制**: 限制并行抓取数量和每个请求的区块数量，防止过载。
* **信任优化**: 依赖于法定数量验证的提交是可信的，从而减少了重新验证的开销。
* **状态管理**: 精确地跟踪已调度、已抓取和已同步的提交范围，以确保数据连续性。

这个模块对于 Sui 节点在网络中断或崩溃后**快速恢复并重新融入共识**至关重要。

---
./core/src/transaction_certifier.rs
---

### `TransactionCertifier` 模块的职责：

`TransactionCertifier` 是 Sui 共识中处理**事务认证**的关键组件。它的主要职责是：

1.  **认证事务**: 根据共识协议规则（即区块因果历史中的投票）判断事务是否被足够多的验证者“接受”或“拒绝”。
2.  **快速路径执行**: 将已认证的区块（包含事务）发送到“快速路径”（fastpath）输出通道，以便它们可以被提前执行，从而降低延迟。
3.  **跟踪投票**: 记录本节点对事务的拒绝投票，并聚合其他验证者对事务的接受/拒绝投票。
4.  **垃圾回收**: 定期清理过时的内部状态。

### 1. `TransactionCertifier` 结构体

* **作用**: 管理事务认证的全生命周期。
* **字段**:
    * `certifier_state: Arc<RwLock<CertifierState>>`: 认证器状态的读写锁包装，这是所有认证逻辑和数据存储的核心。
    * `dag_state: Arc<RwLock<DagState>>`: DAG 状态的读写锁包装，用于获取区块的连通性信息和 GC 轮次。
    * `certified_blocks_sender: UnboundedSender<CertifiedBlocksOutput>`: 一个无界通道的发送端，用于将已认证的区块（包含事务）发送到“快速路径”供后续处理。

### 2. `impl TransactionCertifier` 方法

* **`pub(crate) fn recover(&self, block_verifier: &impl BlockVerifier)`**:
    * **作用**: 从持久化存储中恢复 `TransactionCertifier` 的内部状态。这在节点启动时非常重要。
    * **逻辑**:
        * 获取 `certifier_state` 的写锁和 `dag_state` 的读锁。
        * 获取当前的 `gc_round`（垃圾回收轮次）。
        * 遍历委员会中的所有验证者。
        * 对于每个验证者，从 `dag_state` 中获取其在 `gc_round` 之后缓存的所有区块。
        * 对这些区块进行**重新验证**：
            * 如果是硬链接（`dag_state.is_hard_linked()`），则认为其无需重新验证事务投票。
            * 否则，调用 `block_verifier.verify_and_vote()` 来重新验证区块并获取其拒绝投票。如果验证失败，会 panic。
        * 调用 `certifier_state.add_voted_blocks()` 将这些区块（及其拒绝投票）添加到认证器状态中，并发送已认证的区块。

* **`pub(crate) fn add_voted_blocks(&self, voted_blocks: Vec<(VerifiedBlock, Vec<TransactionIndex>)>)`**:
    * **作用**: 处理一组已投票的区块（即本节点接收并验证过的区块），将其添加到认证器状态中。
    * **逻辑**: 获取 `certifier_state` 的写锁，并调用 `certifier_state.add_voted_blocks()`（私有方法），然后将返回的已认证区块发送到 `certified_blocks_sender`。

* **`pub(crate) fn add_proposed_block(&self, proposed_block: VerifiedBlock)`**:
    * **作用**: 处理本节点提议的区块，将其添加到认证器状态中以聚合接受投票。
    * **逻辑**: 获取 `certifier_state` 的写锁，并调用 `certifier_state.add_proposed_block()`（私有方法），然后将返回的已认证区块发送到 `certified_blocks_sender`。

* **`fn send_certified_blocks(&self, certified_blocks: Vec<CertifiedBlock>)`**:
    * **作用**: 将已认证的区块发送到 `certified_blocks_sender` 通道。
    * **逻辑**: 如果 `certified_blocks` 不为空，则尝试通过 `send` 方法将其发送出去。如果发送失败（例如通道已满或关闭），记录警告日志。

* **`pub(crate) fn get_block_transaction_votes(&self, block_refs: Vec<BlockRef>) -> Vec<BlockTransactionVotes>`**:
    * **作用**: 获取给定区块引用中包含的事务的投票信息（主要是拒绝投票）。
    * **逻辑**: 遍历 `block_refs`。如果区块轮次小于或等于 `certifier_state.gc_round`，则跳过。从 `certifier_state.votes` 中获取对应区块的 `VoteInfo`。如果存在拒绝投票，则将其收集到 `BlockTransactionVotes` 结构体中并返回。

* **`pub(crate) fn run_gc(&self)`**:
    * **作用**: 运行垃圾回收，清理 `TransactionCertifier` 内部的过时状态。
    * **逻辑**: 获取 `dag_state` 的读锁获取最新的 `gc_round`，然后获取 `certifier_state` 的写锁，并调用 `certifier_state.update_gc_round(gc_round)` 来实际执行清理。

### 3. `CertifierState` 结构体

* **作用**: 维护 `TransactionCertifier` 的核心状态，包括对区块和事务的投票信息。
* **字段**:
    * `context: Arc<Context>`: 节点的全局上下文。
    * `votes: BTreeMap<BlockRef, VoteInfo>`: 存储每个区块（由 `BlockRef` 标识）的 `VoteInfo`。
    * `gc_round: Round`: 当前的垃圾回收轮次，用于清理过时的投票信息。

### 4. `impl CertifierState` 方法

* **`fn add_voted_block(...) -> Vec<CertifiedBlock>`**:
    * **作用**: 处理单个已投票的区块（本节点接收并验证的区块）。
    * **逻辑**:
        * **GC 过滤**: 如果区块轮次小于或等于 `gc_round`，则跳过。
        * **检查重复**: 如果区块已存在于 `votes` 中，则跳过。
        * **存储区块**: 将区块本体和本节点的拒绝投票存储到 `votes` 映射中。
        * **聚合拒绝投票**: 遍历 `voted_block` 包含的 `BlockTransactionVotes`，聚合其他验证者对这些事务的拒绝投票。
        * **检查认证**: 调用 `vote_info.take_certified_output()` 检查该区块是否已被认证。如果已认证，则收集到 `certified_blocks` 列表中。
        * 返回已认证区块列表。

* **`fn add_proposed_block(...) -> Vec<CertifiedBlock>`**:
    * **作用**: 处理本节点提议的区块，聚合接受投票。
    * **逻辑**:
        * **GC 过滤**: 如果区块轮次低于 `gc_round + 2`，则跳过。
        * **断言存在**: 断言 `votes` 中已包含本节点提议的区块。
        * **聚合接受投票**: 遍历提议区块的祖先（这些祖先在提案中被接受），并为这些祖先（它们也是其他区块的提议者）的 `VoteInfo` 增加“接受投票”。
        * **检查认证**: 调用 `target_vote_info.take_certified_output()` 检查这些祖先区块是否因接受投票达到法定数量而被认证。

* **`fn update_gc_round(&mut self, gc_round: Round)`**:
    * **作用**: 更新 `gc_round` 并清理 `votes` 映射中过时的条目（轮次小于或等于 `gc_round` 的区块）。

### 5. `VoteInfo` 结构体

* **作用**: 存储单个区块的投票信息，包括本节点的投票和来自其他验证者的聚合投票。
* **字段**:
    * `block: Option<VerifiedBlock>`: 区块的内容（可能尚未收到）。
    * `own_reject_txn_votes: Vec<TransactionIndex>`: 本节点对该区块中事务的拒绝投票。
    * `accept_block_votes: StakeAggregator<QuorumThreshold>`: 聚合的来自其他验证者对该区块的接受投票。
    * `reject_txn_votes: BTreeMap<TransactionIndex, StakeAggregator<QuorumThreshold>>`: 聚合的来自其他验证者对该区块中每个事务的拒绝投票。
    * `is_certified: bool`: 标志，指示该区块是否已被认证。

### 6. `impl VoteInfo` 方法

* **`fn take_certified_output(&mut self, committee: &Committee) -> Option<CertifiedBlock>`**:
    * **作用**: 检查该 `VoteInfo` 对应的区块是否已满足认证条件，如果满足且未被认证过，则返回一个 `CertifiedBlock`，并标记为已认证。
    * **逻辑**:
        * 如果已认证，或区块内容未收到，或接受投票未达到法定数量，则返回 `None`。
        * 遍历区块中的每个事务，检查它是否被拒绝（有法定拒绝投票）。
        * **认证条件**: 一个区块被认证，需要其自身获得法定数量的接受投票，并且其包含的**所有事务**要么被接受（接受投票足够），要么被拒绝（拒绝投票达到法定数量）。如果一个事务既没有被接受也没有被拒绝，那么整个区块就不能被认证。
        * 如果区块被认证，则将 `is_certified` 设为 `true`，并返回 `CertifiedBlock`。

### 总结：

`TransactionCertifier` 实现了**事务的“快速路径”和“最终确定性”**。其核心在于：

* **基于投票的事务认证**: 利用共识区块的因果历史中包含的隐式接受投票和显式拒绝投票来确定事务的状态。
* **双重状态追踪**: 维护区块的接受投票和事务的拒绝投票，以进行精细的认证。
* **快速路径优化**: 将已认证的区块发送给执行层进行提前执行，以减少延迟。
* **与最终确定性协同**: 确保即使快速路径不执行，事务也能在共识提交后最终确定。
* **内存管理**: 通过 `gc_round` 进行垃圾回收，清理过时的投票信息，防止内存无限增长。
* **鲁棒性**: 处理各种边缘情况，如缺失区块、无效投票、时间戳偏差等。

这个模块对于 Sui 实现高吞吐量、低延迟和强最终确定性的共识协议至关重要。

---
./core/src/core.rs
---
它包含了大部分共识协议状态机和决策逻辑，负责驱动区块的提议、提交、验证和状态管理。

### 1. `Core` 结构体

* **作用**: `Core` 是一个 Sui 验证者节点内部共识状态机和算法的中央枢纽。它封装了所有共识决策、状态更新和与其他内部组件的交互。
* **关键字段**:
    * `context: Arc<Context>`: 节点的全局上下文，提供委员会、参数、协议配置、度量和时钟等共享信息。
    * `transaction_consumer: TransactionConsumer`: 用于从内存池获取待处理交易。
    * `transaction_certifier: TransactionCertifier`: 事务认证器，用于判断事务是否被接受或拒绝。
    * `block_manager: BlockManager`: 区块管理器，负责处理区块的依赖关系、悬挂和解除悬挂。
    * `subscriber_exists: bool`: 标志，指示是否有活跃的订阅者在监听本节点提议的区块。
    * `propagation_delay: Round`: 区块传播到法定数量对等节点的估计延迟（以轮次计），用于决定是否暂停提议。
    * `committer: UniversalCommitter`: 通用提交器，负责根据共识规则决定哪些区块可以被提交。
    * `last_signaled_round: Round`: 最后一次发送新轮次信号的轮次。
    * `last_included_ancestors: Vec<Option<BlockRef>>`: 每个 Authority 最后一次被包含在提案中的祖先区块引用，用于优化后续提案的祖先选择。
    * `last_decided_leader: Slot`: 最后一次被提交算法确定为领导者的 `(Round, AuthorityIndex)` 槽位。
    * `leader_schedule: Arc<LeaderSchedule>`: 领导者调度器，决定每个轮次的领导者。
    * `commit_observer: CommitObserver`: 观察共识提交，并将提交的子 DAG 发送到输出通道。
    * `signals: CoreSignals`: Core 发送出站信号（如新区块、新轮次）的发送器。
    * `block_signer: ProtocolKeyPair`: 用于签署本节点提议区块的私钥。
    * `dag_state: Arc<RwLock<DagState>>`: DAG（有向无环图）的内存状态。
    * `last_known_proposed_round: Option<Round>`: 节点最后已知自己提议区块的轮次。用于在失忆恢复时防止重复提议。
    * `ancestor_state_manager: AncestorStateManager`: 祖先状态管理器，根据区块传播得分来评估 Authority 的质量。
    * `round_tracker: Arc<RwLock<PeerRoundTracker>>`: 跟踪对等节点最高已接收/接受轮次。

### 2. `impl Core` 方法
#### 2.1 `new` 方法
`pub(crate) fn new(...) -> Self`
* **作用**: `Core` 结构体的构造函数，负责初始化共识状态机的所有组件。
* **参数**: 接收 `context`、`leader_schedule`、`transaction_consumer` 等所有 `Core` 依赖的组件。
* **逻辑**:
    * 初始化 `UniversalCommitter`，设置领导者数量 (`mysticeti_num_leaders_per_round`) 和管道化 (`pipeline`)。
    * 从 `dag_state` 恢复 `last_decided_leader` 和 `last_proposed_block`。
    * 恢复 `last_included_ancestors`：根据最后提议的区块的祖先来初始化，优化后续提案的祖先选择。
    * 初始化 `ancestor_state_manager` 并设置传播得分。
    * `recover()`：调用私有 `recover` 方法进行从存储中恢复状态。
    * 返回一个 `Core` 实例。

#### 2.2 recover
`fn recover(mut self) -> Self`
* **作用**: 从持久化存储中恢复 `Core` 的内部状态。这在节点启动时运行。
* **逻辑**:
    * **时间同步**: 确保本地时钟不早于其祖先区块的最新时间戳。如果 `consensus_median_based_commit_timestamp` 启用，则不等待，否则等待。
    * **尝试提交**: 调用 `self.try_commit(vec![])` 尝试提交已存储的区块，这会推进 `last_decided_leader` 和 `dag_state`。
    * **尝试提议**: 调用 `self.try_propose(true)` 尝试提议一个新区块（`force=true` 强制提议），确保即使恢复后没有新事件，节点也能立即提议。
    * 如果未提议新区块，则重新广播最后提议的区块（`signals.new_block`），确保活性。
    * **信号新轮次**: 调用 `self.try_signal_new_round()`，根据恢复后的 `threshold_clock_round` 发送新轮次信号。
    * 打印恢复完成日志。
    * 返回恢复后的 `Core` 实例。

#### 2.3 add_blocks
`pub(crate) fn add_blocks(...) -> ConsensusResult<BTreeSet<BlockRef>>`
* **作用**: 将新收到的区块（可能来自网络或同步器）添加到 Core 中进行处理。
* **参数**: `blocks: Vec<VerifiedBlock>`。
* **逻辑**:
    * 调用 `self.block_manager.try_accept_blocks(blocks)` 尝试接受区块。这会处理区块依赖、悬挂和解除悬挂。返回 `accepted_blocks` 和 `missing_block_refs`。
    * **度量**: 记录处理的区块数量。
    * 如果 `accepted_blocks` 不为空：
        * 调用 `self.try_commit(vec![])` 尝试提交新接受的区块。
        * 调用 `self.try_propose(false)` 尝试提议新区块。
        * 调用 `self.try_signal_new_round()` 信号新轮次。
    * 返回仍然缺失的区块引用 `missing_block_refs`，供同步器进一步处理。

#### 2.4 add_certified_commits
`pub(crate) fn add_certified_commits(...) -> ConsensusResult<BTreeSet<BlockRef>>`
* **作用**: 处理通过 `CommitSyncer` 同步来的已认证提交。
* **参数**: `certified_commits: CertifiedCommits`。
* **逻辑**:
    * 调用 `self.filter_new_commits()` 过滤掉已经提交过的 `CertifiedCommit`，并检查提交链的连续性。
    * **接受已认证区块**: 调用 `self.block_manager.try_accept_committed_blocks(votes)` 将已认证提交中包含的区块添加到 `BlockManager`。
    * **尝试提交**: 调用 `self.try_commit(commits)`，传入过滤后的 `CertifiedCommit`，强制提交这些权威的提交。
    * **尝试提议**: 调用 `self.try_propose(false)` 尝试提议新区块。
    * **信号新轮次**: 调用 `self.try_signal_new_round()`。
    * 返回处理过程中发现的缺失区块引用。

#### 2.5 check_block_refs
`pub(crate) fn check_block_refs(...) -> ConsensusResult<BTreeSet<BlockRef>>`
* **作用**: 检查一组 `BlockRef` 是否已存在于 Core 的 `dag_state` 或 `block_manager` 的悬挂队列中。
* **逻辑**: 调用 `self.block_manager.try_find_blocks(block_refs)`，并返回所有仍然缺失的区块引用。

#### 2.6 try_signal_new_round
`fn try_signal_new_round(&mut self)`
* **作用**: 私有辅助函数，如果阈值时钟轮次 (`threshold_clock_round`) 已经前进，则发送新的轮次信号。
* **逻辑**:
    * 获取 `threshold_clock_round`。
    * 如果它大于 `last_signaled_round`，则通过 `signals.new_round()` 发送信号，并更新 `last_signaled_round`。
    * 更新 `threshold_clock_round` 指标。

#### 2.7 new_block
`pub(crate) fn new_block(...) -> ConsensusResult<Option<VerifiedBlock>>`
* **作用**: 用于在领导者超时时创建新区块。
* **参数**: `round: Round` (目标轮次), `force: bool` (是否强制提议，忽略检查)。
* **逻辑**:
    * 如果当前轮次已超过 `last_proposed_round`，则增加 `leader_timeout_total` 指标。
    * 调用 `self.try_propose(force)` 尝试提议区块。
    * 调用 `self.try_signal_new_round()`。
    * 返回提议的区块（如果成功）。

#### 2.8 filter_new_commits
`fn filter_new_commits(...) -> ConsensusResult<Vec<CertifiedCommit>>`
* **作用**: 私有辅助函数，过滤掉列表中已经提交过的 `CertifiedCommit`，并检查提交链的连续性。
* **逻辑**:
    * 根据 `last_commit_index` 过滤掉旧的提交。
    * **连续性检查**: 断言第一个新提交的索引是 `last_commit_index + 1`，如果不是，则表示存在间隙，返回 `ConsensusError::UnexpectedCertifiedCommitIndex`。

#### 2.9 try_propose
`fn try_propose(&mut self, force: bool) -> ConsensusResult<Option<VerifiedBlock>>`
* **作用**: 尝试提议一个新区块。这是区块生产的核心。
* **参数**: `force: bool`，如果为 `true`，则忽略 `should_propose` 的一些检查（如领导者存在性、最小轮次延迟）。
* **逻辑**:
    * 调用 `self.should_propose()` 检查是否可以提议。
    * 调用 `self.try_new_block(force)` 实际创建区块。
    * **发送信号**: 如果区块创建成功，通过 `signals.new_block()` 广播新区块。
    * **副作用**: 提议新区块后，可能会触发 `try_commit()`，因为新区块的加入可能使得新的提交条件满足。
    * 更新 `proposed_block_ancestors`、`proposed_block_size`、`block_proposal_interval` 等度量。
    * 将区块接受到 `block_manager` 和 `dag_state`。
    * 将区块添加到 `transaction_certifier`。
    * 更新 `round_tracker`。
    * 返回提议的 `ExtendedBlock`。

#### 2.10 try_new_block
`fn try_new_block(&mut self, force: bool) -> Option<ExtendedBlock>`
* **作用**: 尝试创建下一个轮次的新区块。
* **逻辑**:
    * 获取 `threshold_clock_round`。如果小于或等于 `last_proposed_block().round()`，则不创建新区块。
    * **非强制模式下的检查**: 如果 `!force`：
        * 调用 `self.leaders_exist(quorum_round)` 检查上一轮的领导者是否存在。
        * 检查是否已通过 `min_round_delay`。
    * **选择祖先**: 调用 `self.smart_ancestors_to_propose()` 选择要包含在区块中的祖先区块（这是一个智能选择过程）。
    * **祖先数量检查**: 如果没有找到足够的祖先区块，且不是强制提议，则不创建区块。
    * **处理排除祖先**: 截断 `excluded_and_equivocating_ancestors` 的数量。
    * **更新 `last_included_ancestors` 记录本次提议中包含的祖先。
    * **度量**: 记录 `block_proposal_leader_wait_ms`、`proposed_block_ancestors` 等指标。
    * **获取交易**: 从 `transaction_consumer` 获取要包含在区块中的交易。
    * **获取提交投票**: 从 `dag_state` 获取要包含在区块中的提交投票。
    * **事务投票**: 从 `transaction_certifier` 获取事务投票。
    * **创建区块**: 根据协议配置（`mysticeti_fastpath`）创建 `BlockV2` 或 `BlockV1`。
    * **签名和序列化**: 对区块进行签名和序列化。
    * 更新 `proposed_block_size` 和 `block_proposal_interval` 度量。
    * 返回创建的 `ExtendedBlock`。

#### 2.11 try_commit
`fn try_commit(...) -> ConsensusResult<Vec<CommittedSubDag>>`
* **作用**: 运行提交规则，尝试提交 DAG 中可以提交的区块，将它们转化为 `CommittedSubDag`。
* **参数**: `certified_commits: Vec<CertifiedCommit>` (可选，如果提供，则优先提交这些)。
* **逻辑**:
    * **处理 `certified_commits` 调用 `self.try_decide_certified()` 优先处理通过 `CommitSyncer` 同步来的已认证提交。
    * **接受已提交区块**: 调用 `self.block_manager.try_accept_committed_blocks()` 将这些已认证提交中的区块接受到 `BlockManager`。
    * **常规提交决策**: 如果没有通过 `certified_commits` 提交任何内容，则调用 `self.committer.try_decide(self.last_decided_leader)` 来运行通用提交器，根据共识规则决定新的提交。
    * **截断决策**: 如果 `decided_leaders` 的数量超过 `commits_until_update`，则进行截断。
    * **更新 `last_decided_leader` 更新最后被提交的领导者。
    * **处理提交**: 调用 `self.commit_observer.handle_commit(sequenced_leaders)` 处理已提交的领导者，这将生成 `CommittedSubDag` 并发送到输出通道。
    * **更新 DAG 状态**: 调用 `self.dag_state.write().add_scoring_subdags()` 更新 DAG 状态的评分。
    * **GC 触发**: 调用 `self.block_manager.try_unsuspend_blocks_for_latest_gc_round()`，触发因 GC 轮次推进而解除悬挂的区块。
    * **通知消费者**: 调用 `self.transaction_consumer.notify_own_blocks_status()` 通知事务消费者本节点提议的区块状态。
    * 返回已提交的 `CommittedSubDag` 列表。

#### 2.12 get_missing_blocks
`pub(crate) fn get_missing_blocks(&self) -> BTreeSet<BlockRef>`
* **作用**: 返回 `block_manager` 中所有当前缺失的区块引用。
* **逻辑**: 调用 `self.block_manager.missing_blocks()`。

#### 2.13 set_subscriber_exists
`pub(crate) fn set_subscriber_exists(&mut self, exists: bool)`
* **作用**: 设置 `subscriber_exists` 标志，指示是否有活跃的区块订阅者。
* **逻辑**: 更新内部字段，并记录日志。

#### 2.14 set_propagation_delay
`pub(crate) fn set_propagation_delay(&mut self, delay: Round)`
* **作用**: 设置区块传播到法定数量对等节点的延迟估计。
* **逻辑**: 更新内部字段，并记录日志。

#### 2.15 set_last_known_proposed_round
`pub(crate) fn set_last_known_proposed_round(&mut self, round: Round)`
* **作用**: 设置本节点最后已知提议区块的轮次，用于失忆恢复和防止重复提议。
* **逻辑**: 更新内部字段，并断言该方法只被调用一次。

#### 2.16 should_propose
`pub(crate) fn should_propose(&self) -> bool`
* **作用**: 判断本节点是否应该提议新区块。
* **逻辑**: 包含一系列检查：
    * `!self.subscriber_exists`: 如果没有订阅者，则跳过提议。
    * `self.propagation_delay > self.context.parameters.propagation_delay_stop_proposal_threshold`: 如果传播延迟过高，则跳过提议。
    * `clock_round <= last_known_proposed_round`: 如果当前时钟轮次不高于最后已知提议轮次，则跳过提议（防止重复）。
    * 更新 `core_skipped_proposals` 指标。

#### 2.17 try_decide_certified
`fn try_decide_certified(...) -> Vec<(DecidedLeader, CertifiedCommit)>`
* **作用**: 私有辅助函数，尝试提交通过 `CommitSyncer` 同步来的已认证提交。
* **逻辑**: 根据 `limit` 参数（一次最多提交多少个），从 `certified_commits` 列表中取出提交。将这些提交转换为 `DecidedLeader` 并返回，同时更新相关度量。

#### 2.18 smart_ancestors_to_propose
`fn smart_ancestors_to_propose(...) -> (Vec<VerifiedBlock>, BTreeSet<BlockRef>)`
* **作用**: 智能选择下一轮提案的祖先区块，考虑 Authority 的传播得分和是否强制提议。
* **逻辑**:
    * 从 `dag_state` 获取所有 Authority 在 `clock_round` 之前的最新区块。
    * `AncestorStateManager` 更新祖先区块的传播状态。
    * **包含本节点最后提议的区块**。
    * **排除低分祖先**: 遍历其他 Authority 的祖先。根据 `ancestor_state_map` 中的 `AncestorState`（`Include` 或 `Exclude`）和传播得分，决定是否将其包含在提案中。
    * **强制提议逻辑**: 如果 `force` 为 `true`，则会忽略部分排除条件，尽可能多地包含祖先。
    * **平局处理**: 如果没有足够多的高分祖先形成法定数量，且不是强制提议，则暂停提议。
    * **处理排除祖先**: 最终，返回要包含在提案中的祖先区块 (`ancestors_to_propose`) 和被排除的祖先区块 (`excluded_and_equivocating_ancestors`)。被排除的祖先也会被广播出去。

#### 2.19 leaders_exist
`fn leaders_exist(&self, round: Round) -> bool`
* **作用**: 检查指定轮次的所有领导者区块是否都已存在于 `dag_state`。
* **逻辑**: 遍历 `self.committer.get_leaders()` 返回的领导者，检查 `dag_state.contains_cached_block_at_slot()`。

#### 2.20 leaders
`fn leaders(&self, round: Round) -> Vec<Slot>` 返回指定轮次的所有领导者槽位。
`fn first_leader(&self, round: Round) -> AuthorityIndex` 返回指定轮次的第一个领导者 Authority。
`fn last_proposed_timestamp_ms()` / `last_proposed_round()` / `last_proposed_block()` 辅助函数，获取本节点最后提议区块的时间戳、轮次和区块本身。

### 总结：

`Core` 模块是 Sui 共识协议的**大脑**：

* **状态管理**: 维护和更新 DAG 状态。
* **区块生命周期**: 协调区块的接收、验证、悬挂和最终接受。
* **提交决策**: 根据共识规则和提交同步器的输入，决定哪些区块序列可以被提交。
* **区块提议**: 根据网络状态、领导者调度和各种条件，智能地提议新区块。
* **故障恢复**: 支持从存储中恢复状态。
* **活性保障**: 通过领导者超时、传播延迟检查和强制提议来确保共识的持续进展。
* **可观测性**: 大量集成日志和 Prometheus 指标，提供对共识内部运行的深入洞察。

